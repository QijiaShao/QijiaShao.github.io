---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
* Welcome, my name is Qijia Shao,  currently a fourth-year Ph.D. student in the [Computer Science Department](https://web.cs.dartmouth.edu/) at [**Dartmouth College**](https://home.dartmouth.edu/). I am a member of the [DartNets](http://dartnets.cs.dartmouth.edu/) (Dartmouth Networking and Ubiquitous Systems) Lab and [RLab](https://rlab.cs.dartmouth.edu/home/) (Dartmouth Reality and Robotics Lab), under the supervision of [**Professor Xia Zhou**](https://www.cs.dartmouth.edu/~xia/) and [**Professor Devin Balkcom**](https://rlab.cs.dartmouth.edu/devin/).

* I received my bachelor's degree with the highest honor from Yingcai Honors College at [University of Electronic Science and Technology of China (UESTC)](https://en.uestc.edu.cn/) in 2018. In 2016, I was an exchange student at EECS department of [National Chiao Tung University](https://www.nctu.edu.tw/en).  I spent my senior year as a research assistant at [Missouri S&T](https://www.mst.edu/), advised by [Professor Y. Rosa Zheng](https://www.lehigh.edu/~yrz218/).

<!-- * During my undergraduate study, I focused more on optimization and statistical signal processing in wireless communication. I am lucky to have been working at National Key Laboratory of Science and Technology on Communication, advised by [Professor Jun Wang](https://scholar.google.com.hk/citations?user=bOK-froAAAAJ&hl=zh-CN) and at Center for Real-time Adaptive Signal Processing, advised by [Professor Y. Rosa Zheng](https://www.lehigh.edu/~yrz218/) -->
<!-- * During my undergraduate study, I am lucky to have been working at Center for Real-time Adaptive Signal Processing, advised by [Professor Y. Rosa Zheng](https://www.lehigh.edu/~yrz218/) -->
                                                                                                                                                                                                                    
I am passionate about solving exciting and impactful real-world challenges. I design, build and evaluate novel and practical mobile systems, especially **sensing systems for physical and physiological signals.** Feel free to contact me if interested in similar topics!

My research focuses on novel and practical mobile systems leveraging the latest technical advances (e.g., **Multimodal Deep Sensing, Mixed Reality/AR/VR, Humanoid Robot**) for physical and physiological signals. I play with various modalities of signals to enable **sensing around humans and robots.**  I have designed and prototyped different mobile systems for  human motion teaching (soft flex/pressure sensors and camera @UbiComp'21),  human activity/behavior monitoring (computational fabrics @UbiComp'19; EMG and impedance sensors @UbiComp'21), cross-medium sensing/localization (laser light @MobiSys'22), interaction (conductive threads @CHI'20). 


<!-- **I am actively seeking for a research intern position for summer 2021. Please ** -->

Recent News
======
* [03/2022] Our paper **"Sunflower: Sensing Underwater Robots From the Air"** has been conditionally accepted to [MobiSys 2022](https://www.sigmobile.org/mobisys/2022/). The first system ever achieves wirelessly localizing underwater robot from the air withut additional infrastructure. Laser light is our secret for cross-medium sensing. Details coming soon!
* [09/2021] We are presenting both **ASLTeach** and **FaceSense** in [UbiComp 2021](https://www.ubicomp.org/ubicomp2021/)!
* [07/2021] Our COVID-motivated paper **"FaceSense: Sensing Face Touch with an Ear-worn System"** is accepted with minor revision by IMWUT (UbiComp2021). It's more than one-year-long effort collaborating with 4 universities. Cheers for the team's hard work during the pandemic!  Please check out our [paper](https://dl.acm.org/doi/pdf/10.1145/3478129) for more details. 
* [06/2021] Started my research internship at Signify (Philips Lighting), focusing on deep learning and sensor data fusion!
* [11/2020] Gave a guest lecture on next-generation mobile  platform -- computational fabrics in CS 69/169 at Dartmouth.
* [10/2020] Our paper **"Teaching American Sign Language in Mixed Reality"** was accepted by IMWUT (UbiComp2021). A great collaboration with researchers from cognitive science and education department at Dartmouth and sign language experts from Gallaudet University. This is our first work on teaching human motion at population scale without coaches. Check out our [paper](http://qijiashao.github.io/files/publications/UbiComp-Teaching American Sign Language in Mixed Reality.pdf) for more details!
* [03/2020] I gave a demo and [talk](https://www.youtube.com/watch?v=lHfvueWdjJQ&t=6s) for our PolarTag paper **"PolarTag: Invisible Data with Light Polarization"** on **[HotMobile 2020](http://www.hotmobile.org/2020/)**. Thanks for everyone's attention and vote! We won the <span style="color:red"> **Best Demo Award** </span>!
<!-- * [02/2020] Received the ACM HotMobile 2020 Student Travel Award. See you at Austin! -->
<!-- * [12/2019] One paper got accepted by **[HotMobile 2020](http://www.hotmobile.org/2020/)**.
* [12/2019] One paper got accepted by **[CHI 2020](https://chi2020.acm.org/)**.
* [09/2019] I presented our fabric paper **"Reconstructing Human Joint Motion with Computational Fabrics"** on **[UbiComp 2019](http://ubicomp.org/ubicomp2019/)** in London. -->




Visitors
=======
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=250&t=n&d=gkUgx_rJxyGnlm9h49vUyEn8lS4ZIy-1rPBbiEUZCKY&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>


