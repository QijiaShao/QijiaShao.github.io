---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

* Welcome, my name is Qijia Shao,  currently a Ph.D. candidate in the [Computer Science Department](https://www.cs.columbia.edu/) at [**Columbia University**](https://www.columbia.edu/). I am a member of the [Mobile X Lab](https://mobilex.cs.columbia.edu/), under the supervision of [**Professor Xia Zhou**](https://www.cs.columbia.edu/~xia/). 


* I received my master's degree from [<span style="color:Green">**Dartmouth College**</span>](https://home.dartmouth.edu/), and my bachelor's degree with the highest honor from Yingcai Honors College at [University of Electronic Science and Technology of China (UESTC)](https://en.uestc.edu.cn/). I was an exchange student in my junior year at EECS department of [National Chiao Tung University](https://www.nctu.edu.tw/en).  I spent my senior year as a research assistant at [Missouri S&T](https://www.mst.edu/), advised by [Professor Y. Rosa Zheng](https://www.lehigh.edu/~yrz218/).

<!-- * During my undergraduate study, I focused more on optimization and statistical signal processing in wireless communication. I am lucky to have been working at National Key Laboratory of Science and Technology on Communication, advised by [Professor Jun Wang](https://scholar.google.com.hk/citations?user=bOK-froAAAAJ&hl=zh-CN) and at Center for Real-time Adaptive Signal Processing, advised by [Professor Y. Rosa Zheng](https://www.lehigh.edu/~yrz218/) -->
                                                                                                                                                                                                                    
I am passionate about solving exciting and impactful real-world challenges. My research is mainly about turning everyday objects in to sensors,  **for sensing physical and physiological signals around humans and robots.**

I play with various modalities of signals from both software and hardware sides. I have designed and prototyped different practical systems  leveraging the latest technical advances (e.g., **Multimodal Deep Sensing, Mixed Reality/AR/VR, Humanoid Robot**) for  human motion teaching (soft flex/pressure sensors and camera @UbiComp'21),  human activity/behavior monitoring/prediction (computational fabrics @UbiComp'19; EMG and impedance sensors @UbiComp'21, N-euro Predictor@UbiComp'23), localization and tracking(light @MobiSys'22, @NSDI'24), and interactions (conductive threads @CHI'20). Feel free to contact me if interested in similar topics!


<!-- **I am actively seeking for a research intern position for summer 2021. Please ** -->

Recent News
======
* [09/2023] Happy and honored to serve as a [JEDI Ambassador](https://www.ubicomp.org/ubicomp-iswc-2023/accessibility/jedi-ambassadors/) for UbiComp/ISWC 2023. I eagerly anticipate collaborating with the JEDI chairs to ensure UbiComp/ISWC 2023 is both accessible and inclusive for all attendees. See you in Cancun!
* [07/2023] Our paper **"N-euro Predictor: A Neural Network Approach for Smoothing and Predicting Motion Trajectory"** has been accepted by IMWUT/[UbiComp2023](https://www.ubicomp.org/ubicomp-iswc-2023/). This paper highlights our efforts in addressing the motion-to-photo latency and jitter in AR/VR applications by utilizing prediction techniques. The research was conducted during my internship at Snap. Check out the short [intro video](https://www.youtube.com/watch?v=oWUvgxlaNUM&list=PLqhXYFYmZ-VdTsnSwophLk4-157aPiJwf&index=34) here. More details to come!
<!-- [07/2023] Our paper **"Catch Me If You Can: Laser Tethering with Highly Mobile Targets"** has been accepted by [NSDI 2024](https://www.usenix.org/conference/nsdi24). We build a generic framework that tightly integrates laser steering with optical tracking to maintain laser connectivity with high-velocity targets, creating a constantly connected laser tether between the Lasertag core unit and a remote target. I deployed the motion prediction algorithm in the system to  quadruple the tracking rate. Check out the [demo video](https://vimeo.com/xresearch/cmiyc) here. More details to come! -->
* [06/2023] Started my research internship at Samsung Research, where I am primarily concentrating on mitigating the cross-user performance variation in physiological signal sensing for health applications.
<!-- * [09/2022] Finished my internship at Snap Research. -->
* [06/2022] I graduated from Dartmouth with a master's degree (surprisingly) and is moving to Columbia with Xia. Will miss here Hanover!
* [05/2022] Started my research internship at Snap Research, working on reducing the motion-to-photon latency for enabling various cool interacvtive systems. 
* [03/2022] Our paper **"Sunflower: Locating Underwater Robots From the Air"** has been conditionally accepted to [MobiSys 2022](https://www.sigmobile.org/mobisys/2022/). The first system ever achieves wirelessly localizing underwater robots from the air withut additional infrastructure. Laser light is our secret for cross-medium sensing. Please check out our demo video [here!](https://www.youtube.com/watch?v=ofpqm2G2s_U)
* [09/2021] We are presenting both **ASLTeach** and **FaceSense** in [UbiComp 2021](https://www.ubicomp.org/ubicomp2021/)!
* [07/2021] Our COVID-motivated paper **"FaceSense: Sensing Face Touch with an Ear-worn System"** is accepted with minor revision by IMWUT (UbiComp2021). It's more than one-year-long effort collaborating with 4 universities. Cheers for the team's hard work during the pandemic!  Please check out our [paper](https://dl.acm.org/doi/pdf/10.1145/3478129) for more details. 
* [06/2021] Started my research internship at Signify (Philips Lighting), focusing on deep learning and sensor data fusion!
* [11/2020] Gave a guest lecture on next-generation mobile  platform -- computational fabrics in CS 69/169 at Dartmouth.
* [10/2020] Our paper **"Teaching American Sign Language in Mixed Reality"** was accepted by [IMWUT]((https://dl.acm.org/doi/10.1145/3432211)) (UbiComp2021). A great collaboration with researchers from cognitive science and education department at Dartmouth and sign language experts from Gallaudet University. This is our first work on teaching human motion at population scale without coaches. Check out the [presentation](https://www.youtube.com/watch?v=695M7eGxZJ4) for more details!
<!-- * [03/2020] I gave a demo and [talk](https://www.youtube.com/watch?v=lHfvueWdjJQ&t=6s) for our PolarTag paper **"PolarTag: Invisible Data with Light Polarization"** on [HotMobile 2020](http://www.hotmobile.org/2020/). Thanks for everyone's attention and vote! We won the <span style="color:red"> **Best Demo Award** </span>! -->


<!-- * [02/2020] Received the ACM HotMobile 2020 Student Travel Award. See you at Austin! -->
<!-- * [12/2019] One paper got accepted by **[HotMobile 2020](http://www.hotmobile.org/2020/)**.
* [12/2019] One paper got accepted by **[CHI 2020](https://chi2020.acm.org/)**.
* [09/2019] I presented our fabric paper **"Reconstructing Human Joint Motion with Computational Fabrics"** on **[UbiComp 2019](http://ubicomp.org/ubicomp2019/)** in London. -->




Visitors
=======
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=250&t=n&d=gkUgx_rJxyGnlm9h49vUyEn8lS4ZIy-1rPBbiEUZCKY&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>


